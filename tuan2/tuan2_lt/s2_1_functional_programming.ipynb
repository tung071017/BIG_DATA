{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqibUTFn1i_8"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvPrXLOu1qj5",
        "outputId": "6cfc195c-0315-4897-b53d-4cf8cd8ab6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=90ec5652aa4a1b2f28019bf8b4a5283736cfe8e4e9387535cdc718761fb05ab0\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nz61YZEv-wgf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/04 00:37:54 WARN Utils: Your hostname, codespaces-dc2d55 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
            "24/04/04 00:37:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/04/04 00:37:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/04/04 00:37:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "24/04/04 00:37:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/04 00:38:15 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "import pyspark as sp\n",
        "sc = sp.SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmxK_VAr9UZC"
      },
      "source": [
        "# Introduction to Big Data analysis with Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMlXIGkX9UZE"
      },
      "source": [
        "##  Understanding SparkContext\n",
        "`SparkContext` is an entry point to interact with underlying Spark functionality. <br>\n",
        "\n",
        "An entry point is where control is transferred from the Operating system to the provided program. In simpler terms, it's like a key to your house. Without the key you cannot enter the house, similarly, without an entry point, you cannot run any PySpark jobs. **You can access the SparkContext in the PySpark shell as a variable named `sc`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpklurB69UZG",
        "outputId": "fc047818-fe81-4580-b03a-a122ba73434a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The version of Spark Context in the PySpark shell is 3.5.1\n"
          ]
        }
      ],
      "source": [
        "# Print the version of SparkContext\n",
        "print(\"The version of Spark Context in the PySpark shell is\", sc.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XPIF1DL9UZI"
      },
      "source": [
        "## Interactive Use of PySpark\n",
        "Spark comes with an interactive Python shell in which PySpark is already installed in it. PySpark shell is useful for basic testing and debugging and it is quite powerful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOx-WfeJ9UZI"
      },
      "source": [
        "## Loading data in PySpark shell\n",
        "PySpark using SparkContext by two different methods.\n",
        "* The first is the SparkContext’s `parallelize()` method on a list.\n",
        "* The second is the SparkContext’s `textFile()` method on a file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R_cc9Ewv9UZJ"
      },
      "outputs": [],
      "source": [
        "# Create a Python list of numbers from 1 to 100\n",
        "numb = range(1, 100)\n",
        "\n",
        "# Load the list into PySpark\n",
        "spark_data = sc.parallelize(numb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9vNdFPhP9UZJ"
      },
      "outputs": [],
      "source": [
        "# file path\n",
        "file_path = \"5000_points.txt\"\n",
        "# Load a local file into PySpark shell\n",
        "lines = sc.textFile(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEmfjU489UZJ"
      },
      "source": [
        "## Use of lambda() with map()\n",
        "The `map()` function in Python returns a list of the results after applying the given function to each item of a given iterable (list, tuple etc.).\n",
        "\n",
        "The general syntax of `map()` function is `map(fun, iter)`. We can also use lambda functions with `map()`.\n",
        "\n",
        "The general syntax of `map()` function with lambda() is `map(lambda <argument>:<expression>, iter)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFsjni0z9UZJ",
        "outputId": "3c7ec2e2-f336-46b4-8bf5-892b4c7fa858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input list is [3, 5, 6, 8, 10, 12, 27, 31]\n"
          ]
        }
      ],
      "source": [
        "my_list = [3, 5, 6, 8, 10, 12, 27, 31]\n",
        "# Print my_list in the console\n",
        "print(\"Input list is\", my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPfKrYC29UZJ",
        "outputId": "b41bd70f-3ea8-4774-c89a-5905ca6f3b10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The squared numbers are [9, 25, 36, 64, 100, 144, 729, 961]\n"
          ]
        }
      ],
      "source": [
        "# Square all numbers in my_list\n",
        "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
        "\n",
        "# Print the result of the map function\n",
        "print(\"The squared numbers are\", squared_list_lambda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61c4dxlg9UZK"
      },
      "source": [
        "## Use of lambda() with filter()\n",
        "The `filter()` function in Python takes in a function and a list as arguments.\n",
        "\n",
        "The general syntax of the `filter()` function is `filter(function, list_of_input)`.\n",
        "\n",
        "The general syntax of the `filter()` function with `lambda()` is `filter(lambda <argument>:<expression>, list)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFng2DXA9UZK",
        "outputId": "854d06c8-3e60-424d-ae96-42667280cd04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input list is: [1, 10, 2, 100, 3, 1000]\n"
          ]
        }
      ],
      "source": [
        "my_list2 = [1, 10, 2, 100, 3, 1000]\n",
        "\n",
        "# Print my_list2 in the console\n",
        "print(\"Input list is:\", my_list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp5sPxYQ9UZK",
        "outputId": "748a3dd6-ae9f-45e0-a81a-a7c75b29bca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numbers divisible by 10 are: [10, 100, 1000]\n"
          ]
        }
      ],
      "source": [
        "# Filter numbers divisible by 10\n",
        "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
        "\n",
        "# Print the numbers divisible by 10\n",
        "print(\"Numbers divisible by 10 are:\", filtered_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
