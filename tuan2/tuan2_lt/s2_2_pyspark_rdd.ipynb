{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Oq72tRr_Imj"
      },
      "source": [
        "# Programming in PySpark RDD’s\n",
        "\n",
        "## RDDs from Parallelized collections\n",
        "Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "okOhevbD_e3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061e022b-ffb4-4bf1-dc3d-a177f78a4c57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=a8d8c9b293d0db37c14042e6b542264c3b384cd47668c024aed3688673ba4059\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_8DPq4J5_Iml"
      },
      "outputs": [],
      "source": [
        "import pyspark as sp\n",
        "sc = sp.SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39xxuJtW_Imm",
        "outputId": "dba6be09-81fe-4118-ff21-a635243df847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "# Create an RDD from a list of words\n",
        "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
        "\n",
        "# Print out the type of the created object\n",
        "print(\"The type of RDD is\", type(RDD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnFKxAOM_Imn"
      },
      "source": [
        "## RDDs from External Datasets\n",
        "PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HP60epF8_Imn"
      },
      "outputs": [],
      "source": [
        "file_path = '5000_points.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy3lQqJ7_Imn",
        "outputId": "07bf9d35-c383-4f3c-8eb5-3d007c1b650e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file_path is 5000_points.txt\n",
            "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "# Print the file_path\n",
        "print(\"The file_path is\", file_path)\n",
        "\n",
        "# Create a fileRDD from file_path\n",
        "fileRDD = sc.textFile(file_path)\n",
        "\n",
        "# Check the type of fileRDD\n",
        "print(\"The file type of fileRDD is\", type(fileRDD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRLnpKfn_Imn"
      },
      "source": [
        "## Partitions in your data\n",
        "SparkContext's `textFile()` method takes an optional second argument called `minPartitions` for specifying the minimum number of partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSG3bXM8_Imo",
        "outputId": "8e8feb6d-06d7-4799-e952-f30f98f7ac7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions in fileRDD is 2\n",
            "Number of partitions in fileRDD_part is 5\n"
          ]
        }
      ],
      "source": [
        "# Check the number of partitions in fileRDD\n",
        "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
        "\n",
        "# Create a fileRDD_part from file_path with 5 partitions\n",
        "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
        "\n",
        "# Check the number of partitions in fileRDD_part\n",
        "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic RDD Transformations and Actions\n",
        "\n",
        "**Basic RDD Transformations**: map(), filter(), flatMap(), and union()\n",
        "\n",
        "**Basic RDD Actions**: collect(), take(N), fisrt(), and count()"
      ],
      "metadata": {
        "id": "xpe6qzsVDrEp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fel0-yAV_Imp"
      },
      "outputs": [],
      "source": [
        "RDD = sc.parallelize([1,2,3,4])\n",
        "RDD_map = RDD.map(lambda x: x * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UNK6hp69_Imp"
      },
      "outputs": [],
      "source": [
        "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
        "RDD_flatmap = RDD.flatMap(lambda x:x.split(\" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBtmcC2v_Imp"
      },
      "source": [
        "The main difference between `map()` and `flatMap()` is that map() returns an RDD of the same length as the input RDD, with each element transformed by the function, while flatMap() returns an RDD of flattened results, where the function can return zero, one, or multiple values for each element in the input RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OH9g6HwS_Imp"
      },
      "outputs": [],
      "source": [
        "RDD = sc.parallelize([1,2,3,4])\n",
        "RDD_map = RDD.map(lambda x: x > 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qO1b1n5F_Imp"
      },
      "outputs": [],
      "source": [
        "inputRDD = sc.textFile(\"spam.txt\")\n",
        "errorRDD = inputRDD.filter(lambda x: \"error\" in x.split())\n",
        "warningsRDD = inputRDD.filter(lambda x: \"warning\" in x.split())\n",
        "combinedRDD = errorRDD.union(warningsRDD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1syeroAp_Imq"
      },
      "source": [
        "**collect()** return all the elements of the dataset as an array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yyil3gjw_Imq"
      },
      "outputs": [],
      "source": [
        "RDD_map = sc.parallelize([1, 4, 9, 16, 25, 36, 49])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MxvokZ1y_Imq",
        "outputId": "3f198f5a-2162-4a15-ede2-e123ff0486a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25, 36, 49]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "RDD_map.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UhCqDzD_Imq"
      },
      "source": [
        "**take(N)** returns an array with the first N elements of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uhCyIo-x_Imq",
        "outputId": "2d098a27-2c30-4e77-fb0b-7f9a9da62a39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "RDD_map.take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3KKzYFZ_Imr"
      },
      "source": [
        "**first()** prints the first element of the RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qjQm8GbF_Imr",
        "outputId": "59c25ed0-7557-42b5-afd8-d2b40422c59e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "RDD_map.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyjv2OzZ_Imr"
      },
      "source": [
        "**count()** return the number of elements in the RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nssTmDv2_Imr",
        "outputId": "bf4ffafe-ecda-4264-9fdb-c6dba50f71fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "RDD_map.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIYIyAXd_Imr"
      },
      "source": [
        "## Map and Collect\n",
        "The main method by which you can manipulate data in PySpark is using `map()`. The `map()` transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OM38JtDO_Imr"
      },
      "outputs": [],
      "source": [
        "numbRDD = sc.parallelize([1,2,3,4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Fy8L-pSP_Imr",
        "outputId": "191999a8-19d6-4479-dd7b-dd08d535e597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "8\n",
            "27\n",
            "64\n"
          ]
        }
      ],
      "source": [
        "# Create map() transformation to cube numbers\n",
        "cubedRDD = numbRDD.map(lambda x: x**3)\n",
        "\n",
        "# Collect the results\n",
        "numbers_all = cubedRDD.collect()\n",
        "\n",
        "# Print the numbers from numbers_all\n",
        "for numb in numbers_all:\n",
        "    print(numb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6I11ron_Ims"
      },
      "source": [
        "## Filter and Count\n",
        "The RDD transformation `filter()` returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "c1Tf_ajH_Ims"
      },
      "outputs": [],
      "source": [
        "# file path\n",
        "file_path = \"spam.txt\"\n",
        "# Load a local file into PySpark shell\n",
        "fileRDD = sc.textFile(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lt8g9DEH_Ims"
      },
      "outputs": [],
      "source": [
        "# Filter the fileRDD to select lines with Spark keyword\n",
        "fileRDD_filter = fileRDD.filter(lambda line: 'FREE' in line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "id": "gLIUeICK_Ims",
        "outputId": "e5d79e48-5b30-4997-bc11-f8dab8642588",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total number of lines with the keyword Spark is 112\n"
          ]
        }
      ],
      "source": [
        "# How many lines are there in fileRDD?\n",
        "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "l8u9HQs3_Ims",
        "outputId": "2dd94ee4-bbb2-48f7-c92e-204129d66ec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FREE MSG:We billed your mobile number by mistake from shortcode 83332.Please call 08081263000 to have charges refunded.This call will be free from a BT landline\n",
            "FREE camera phones with linerental from 4.49/month with 750 cross ntwk mins. 1/2 price txt bundle deals also avble. Call 08001950382 or call2optout/J MF\n",
            "You'll not rcv any more msgs from the chat svc. For FREE Hardcore services text GO to: 69988 If u get nothing u must Age Verify with yr network & try again\n",
            "Had your mobile 11mths ? Update for FREE to Oranges latest colour camera mobiles & unlimited weekend calls. Call Mobile Upd8 on freefone 08000839402 or 2StopTxt\n"
          ]
        }
      ],
      "source": [
        "# Print the first four lines of fileRDD\n",
        "for line in fileRDD_filter.take(4):\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz9oI5cf_Imt"
      },
      "source": [
        "## Pair RDDs in PySpark\n",
        "key/value pairs: key is the identifier and value is data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m1oXvid_Imt"
      },
      "source": [
        "### Creating pair RDDs\n",
        "Two common ways to create pair RDDs\n",
        " - From a list of key-value tuple\n",
        " - From a regular RDD\n",
        "\n",
        "Get the data into key/value form for paired RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "o1VC7qwZ_Imt"
      },
      "outputs": [],
      "source": [
        "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
        "pairRDD_tuple = sc.parallelize(my_tuple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rvvTQ1jc_Imw"
      },
      "outputs": [],
      "source": [
        "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
        "regularRDD = sc.parallelize(my_list)\n",
        "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tmgmr_0_Imw"
      },
      "source": [
        "### Transformations on pair RDDs\n",
        "All regular transformations work on pair RDD\n",
        "\n",
        "Have to pass functions that operate on key value pairs rather than on individual elements\n",
        "\n",
        "Paired RDD Transformations:\n",
        "  - `reduceByKey(func)`: Combine values with the same key\n",
        "  - `groupByKey()`: Group values with the same key\n",
        "  - `sortByKey()`: Return an RDD sorted by the key\n",
        "  - `join()`: Join two pair RDDs based on their key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TdujQyTT_Imx",
        "outputId": "174fb62c-2556-44d5-a81b-c71c86311467",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ronaldo', 34), ('Messi', 47), ('Neymar', 22)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34),\n",
        "             (\"Neymar\", 22), (\"Messi\", 24)])\n",
        "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\n",
        "pairRDD_reducebykey.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mSPXFIor_Imx",
        "outputId": "3e719569-dba6-495d-eae1-f95e0118e8dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(47, 'Messi'), (34, 'Ronaldo'), (22, 'Neymar')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
        "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iaMMJtob_Imx",
        "outputId": "d1ba2bb8-dae7-4753-eb0d-5ae1dc3aa0b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "US ['JFK', 'SFO']\n",
            "UK ['LHR']\n",
            "FR ['CDG']\n"
          ]
        }
      ],
      "source": [
        "airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US\", \"SFO\")]\n",
        "regularRDD = sc.parallelize(airports)\n",
        "pairRDD_group = regularRDD.groupByKey().collect()\n",
        "for cont, air in pairRDD_group:\n",
        "    print(cont, list(air))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KbQL6Kb2_Imx",
        "outputId": "b5a3c897-a95d-4201-9cc0-afc4a3dd25da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Neymar', (24, 120)), ('Ronaldo', (32, 80)), ('Messi', (34, 100))]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])\n",
        "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])\n",
        "RDD1.join(RDD2).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwhAa8Ev_Imy"
      },
      "source": [
        "## ReduceBykey and Collect\n",
        "reduceByKey() operates on key, value (k,v) pairs and merges the values for each key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Gws3qHT1_Imy",
        "outputId": "9ec82a3e-a113-40c9-e076-28d318d9825c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key 4 has 5 Counts\n",
            "Key 1 has 2 Counts\n",
            "Key 3 has 10 Counts\n"
          ]
        }
      ],
      "source": [
        "# Create PairRDD Rdd with key value pairs\n",
        "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
        "\n",
        "# Apply reduceByKey() operation on Rdd\n",
        "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
        "\n",
        "# Iterate over the result and print the output\n",
        "for num in Rdd_Reduced.collect():\n",
        "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k1Usspr_Imy"
      },
      "source": [
        "## SortBykey and Collect\n",
        "**sortByKey()** returns an RDD sorted by key in ascending or descending order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jOv4he_Y_Imy",
        "outputId": "a8993cfc-c347-4a78-d757-9145ded057ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key 4 has 5 Counts\n",
            "Key 3 has 10 Counts\n",
            "Key 1 has 2 Counts\n"
          ]
        }
      ],
      "source": [
        "# Sort the reduced RDD with the key by descending order\n",
        "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
        "\n",
        "# Iterate over the result and retrieve all the elements of the RDD\n",
        "for num in Rdd_Reduced_Sort.collect():\n",
        "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clmO9_SX_Imy"
      },
      "source": [
        "## Advanced RDD Actions\n",
        "**reduce() action**\n",
        "* `reduce(func)` action is used for aggregating 2 elements of same type) and returns a new element of the same type.\n",
        "* The function should be commutative (changing the order of the operands does not change the result) and associative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "59PuNogr_Imy",
        "outputId": "970de38b-6958-431c-91b0-e27e2cbf1701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "x = [1,3,4,6]\n",
        "RDD = sc.parallelize(x)\n",
        "RDD.reduce(lambda x, y: x+y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWE2p0Ih_Imy"
      },
      "source": [
        "In many cases, it is not advisable to run collect action on RDDs because of the huge size of the data.<br>\n",
        "**saveAsTextFile() action**\n",
        "* `saveAsTextFile()` action saves RDD into a text file inside a directory with each partition as a separate file\n",
        "* `coalesce()` method can be used to save RDD as a single text file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD.saveAsTextFile(\"tempFile\")"
      ],
      "metadata": {
        "id": "Cqs0d0ZuECVf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EwXa2zU4_Imz"
      },
      "outputs": [],
      "source": [
        "RDD.coalesce(1).saveAsTextFile(\"tempFile1.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu7ixJI8_Imz"
      },
      "source": [
        "## Action Operations on pair RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### countByKey() action\n",
        "`countByKey()` only available for type(K,V)\n",
        "\n",
        "`countByKey()` action counts the number of elements for each key<br>\n",
        "\n",
        "**note**: countByKey should only be used on a dataset whose size is small enough to fit in memory."
      ],
      "metadata": {
        "id": "Cryl_OveE8Xu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-rfYWNMK_Imz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7747f00f-1f52-420f-c0a2-7e1045a7bb56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a 2\n",
            "b 1\n"
          ]
        }
      ],
      "source": [
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1),(\"a\", 1)])\n",
        "for kee, val in rdd.countByKey().items():\n",
        "    print(kee, val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qN7LVYe_Imz"
      },
      "source": [
        "### collectAsMap() action\n",
        "\n",
        "`collectAsMap()` return the key-value pairs in the RDD as a dictionary\n",
        "\n",
        "**note**: similar to countByKey, this action should only be used if the resulting data is expected to be small, as all the data is loaded into the memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_iwQhsRA_Im0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ad8650-b1b8-4859-c5c4-21b5c1bca50b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 2, 3: 4}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "sc.parallelize([(1,2),(3,4)]).collectAsMap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFQJQTkv_Im0"
      },
      "source": [
        "## CountingBykeys\n",
        "For many datasets, it is important to count the number of keys in a key/value dataset. For example, counting the number of countries where the product was sold or to show the most popular baby names. In this simple exercise, you'll use the Rdd that you created earlier and count the number of unique keys in that pair RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "C5IQVu0g_Im0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef474f4-6dcc-4826-b6e1-dec6f842e566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The type of total is <class 'collections.defaultdict'>\n",
            "key 1 has 1 counts\n",
            "key 3 has 2 counts\n",
            "key 4 has 1 counts\n"
          ]
        }
      ],
      "source": [
        "# Count the unique keys\n",
        "total = Rdd.countByKey()\n",
        "\n",
        "# What is the type of total?\n",
        "print(\"The type of total is\", type(total))\n",
        "\n",
        "# Iterate over the total and print the output\n",
        "for k, v in total.items():\n",
        "    print(\"key\", k, \"has\", v, \"counts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lMvJPO-_Im0"
      },
      "source": [
        "## Create a base RDD and transform it\n",
        "\n",
        "**Write code that calculates the most common words**<br>\n",
        "Here are the brief steps for writing the word counting program:<br>\n",
        "\n",
        "* Create a base RDD from Complete_Shakespeare.txt file.\n",
        "* Use RDD transformation to create a long list of words from each element of the base RDD.\n",
        "* Remove stop words from your data.\n",
        "* Create pair RDD where each element is a pair tuple of ('w', 1)\n",
        "* Group the elements of the pair RDD by key (word) and add up their values.\n",
        "* Swap the keys (word) and values (counts) so that keys is count and value is the word.\n",
        "* Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\n",
        "* In this first part, you'll create a base RDD from Complete_Shakespeare.txt file and transform it to create a long list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pQFrcwr1_Im0"
      },
      "outputs": [],
      "source": [
        "# file path\n",
        "file_path = \"Complete_Shakespeare.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5NAx1KeC_Im0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdadd798-f2b6-4c1f-cde0-761873d9fadb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in splitRDD: 128576\n"
          ]
        }
      ],
      "source": [
        "# Create a baseRDD from the file path\n",
        "baseRDD = sc.textFile(file_path)\n",
        "\n",
        "# Split the lines of baseRDD into words\n",
        "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
        "\n",
        "# Count the total number of words\n",
        "print(\"Total number of words in splitRDD:\", splitRDD.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGDDQFz_Im0"
      },
      "source": [
        "## Remove stop words and reduce the dataset\n",
        "In the next step, you'll remove stop words from your data. Stop words are common words that are often uninteresting. For example \"I\", \"the\", \"a\" etc., are stop words.\n",
        "\n",
        "After removing stop words, you'll next create a pair RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, pair RDD is composed of `(w, 1)` where `w` is for each word in the RDD and `1` is a number.\n",
        "\n",
        "Finally, you'll combine the values with the same key from the pair RDD.Stop words are common words that are often uninteresting. For example \"I\", \"the\", \"a\" etc., are stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6o4iqXol_Im0"
      },
      "outputs": [],
      "source": [
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',\n",
        "              'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
        "              'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these',\n",
        "              'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
        "              'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
        "              'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before',\n",
        "              'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
        "              'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n",
        "              'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
        "              'can', 'will', 'just', 'don', 'should', 'now']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "aR7TU1Bl_Im1"
      },
      "outputs": [],
      "source": [
        "# Convert the words in lower case and remove stop words from the stop_words curated list\n",
        "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
        "\n",
        "# Create a tuple of the word and 1\n",
        "splitRDD_no_stop_words =splitRDD_no_stop.map(lambda w: (w, 1))\n",
        "\n",
        "# Count of the number of occurences of each word\n",
        "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xffyKSL_Im1"
      },
      "source": [
        "## Print word frequencies\n",
        "You could have retrieved all the elements at once using collect() but it is not recommended. RDDs can be huge: you may run out of memory and crash your computer.\n",
        "\n",
        "For this, first you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count). This way it is easy to sort the RDD based on the key rather than the key using sortByKey operation in PySpark. Finally, you'll return the top 10 words from the sorted RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "hjO99d2I_Im1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3fe3ec-de81-4639-eb4b-dd0360098523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Project', 9)\n",
            "('EBook', 1)\n",
            "('Shakespeare', 12)\n",
            "('use', 38)\n",
            "('anyone', 1)\n",
            "('anywhere', 1)\n",
            "('restrictions', 1)\n",
            "('whatsoever.', 1)\n",
            "('may', 162)\n",
            "('it,', 74)\n"
          ]
        }
      ],
      "source": [
        "# Display the first 10 words and their frequencies from the input RDD\n",
        "for word in resultRDD.take(10):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wNQmSSy2_Im1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef3624f-65ff-4a90-924b-4c30a1f29ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thou,650\n",
            "thy,574\n",
            "shall,393\n",
            "would,311\n",
            "good,295\n",
            "thee,286\n",
            "love,273\n",
            "Enter,269\n",
            "th',254\n",
            "make,225\n"
          ]
        }
      ],
      "source": [
        "# Swap the keys and values from the input RDD\n",
        "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "# Sort the keys in descending order\n",
        "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
        "\n",
        "# Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
        "for word in resultRDD_swap_sort.take(10):\n",
        "    print(\"{},{}\". format(word[1], word[0]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}